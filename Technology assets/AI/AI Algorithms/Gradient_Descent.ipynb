{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ej4TREqtBfg",
        "outputId": "90414370-6fa4-4f04-9d8a-a93caeca571e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: x = [0. 0.], gradient = [ 7. -2.], step_size = 0.01\n",
            "Iteration 1: x = [-0.07  0.02], gradient = [ 8.423928 -2.1     ], step_size = 0.01\n",
            "Iteration 2: x = [-0.15423928  0.041     ], gradient = [10.08073907 -2.22647856], step_size = 0.01\n",
            "Iteration 3: x = [-0.25504667  0.06326479], gradient = [11.96595465 -2.38356377], step_size = 0.01\n",
            "Iteration 4: x = [-0.37470622  0.08710042], gradient = [14.03666881 -2.57521159], step_size = 0.01\n",
            "Iteration 5: x = [-0.51507291  0.11285254], gradient = [16.18466732 -2.80444073], step_size = 0.01\n",
            "Iteration 6: x = [-0.67691958  0.14089695], gradient = [18.20481245 -3.07204526], step_size = 0.01\n",
            "Iteration 7: x = [-0.8589677  0.1716174], gradient = [19.77403916 -3.37470061], step_size = 0.01\n",
            "Iteration 8: x = [-1.05670809  0.20536441], gradient = [20.47517843 -3.70268738], step_size = 0.01\n",
            "Iteration 9: x = [-1.26145988  0.24239128], gradient = [19.91078838 -4.0381372 ], step_size = 0.01\n",
            "Iteration 10: x = [-1.46056776  0.28277265], gradient = [17.91405342 -4.35559022], step_size = 0.01\n",
            "Iteration 11: x = [-1.6397083   0.32632855], gradient = [14.74653025 -4.62675949], step_size = 0.01\n",
            "Iteration 12: x = [-1.7871736   0.37259615], gradient = [11.0738412 -4.8291549], step_size = 0.01\n",
            "Iteration 13: x = [-1.89791201  0.4208877 ], gradient = [ 7.64815791 -4.95404863], step_size = 0.01\n",
            "Iteration 14: x = [-1.97439359  0.47042818], gradient = [ 4.94747632 -5.00793081], step_size = 0.01\n",
            "Iteration 15: x = [-2.02386835  0.52050749], gradient = [ 3.07084459 -5.00672172], step_size = 0.01\n",
            "Iteration 16: x = [-2.0545768   0.57057471], gradient = [ 1.87700425 -4.96800418], step_size = 0.01\n",
            "Iteration 17: x = [-2.07334684  0.62025475], gradient = [ 1.15980396 -4.90618418], step_size = 0.01\n",
            "Iteration 18: x = [-2.08494488  0.66931659], gradient = [ 0.7435641  -4.83125658], step_size = 0.01\n",
            "Iteration 19: x = [-2.09238052  0.71762916], gradient = [ 0.5064615  -4.74950273], step_size = 0.01\n",
            "Iteration 20: x = [-2.09744514  0.76512419], gradient = [ 0.3723617 -4.6646419], step_size = 0.01\n",
            "Iteration 21: x = [-2.10116875  0.8117706 ], gradient = [ 0.29630097 -4.5787963 ], step_size = 0.01\n",
            "Iteration 22: x = [-2.10413176  0.85755857], gradient = [ 0.25255752 -4.49314639], step_size = 0.01\n",
            "Iteration 23: x = [-2.10665734  0.90249003], gradient = [ 0.22668695 -4.40833462], step_size = 0.01\n",
            "Iteration 24: x = [-2.10892421  0.94657338], gradient = [ 0.21066802 -4.32470166], step_size = 0.01\n",
            "Iteration 25: x = [-2.11103089  0.98982039], gradient = [ 0.20007794 -4.24242099], step_size = 0.01\n",
            "Iteration 26: x = [-2.11303167  1.0322446 ], gradient = [ 0.19248974 -4.16157413], step_size = 0.01\n",
            "Iteration 27: x = [-2.11495657  1.07386035], gradient = [ 0.1865761  -4.08219244], step_size = 0.01\n",
            "Iteration 28: x = [-2.11682233  1.11468227], gradient = [ 0.18161282 -4.00428011], step_size = 0.01\n",
            "Iteration 29: x = [-2.11863846  1.15472507], gradient = [ 0.17720529 -3.92782677], step_size = 0.01\n",
            "Iteration 30: x = [-2.12041051  1.19400334], gradient = [ 0.17313852 -3.85281434], step_size = 0.01\n",
            "Iteration 31: x = [-2.12214189  1.23253148], gradient = [ 0.16929512 -3.77922082], step_size = 0.01\n",
            "Iteration 32: x = [-2.12383484  1.27032369], gradient = [ 0.16561067 -3.70702231], step_size = 0.01\n",
            "Iteration 33: x = [-2.12549095  1.30739391], gradient = [ 0.16204942 -3.63619408], step_size = 0.01\n",
            "Iteration 34: x = [-2.12711145  1.34375585], gradient = [ 0.15859113 -3.56671118], step_size = 0.01\n",
            "Iteration 35: x = [-2.12869736  1.37942297], gradient = [ 0.15522393 -3.49854878], step_size = 0.01\n",
            "Iteration 36: x = [-2.1302496   1.41440845], gradient = [ 0.15194049 -3.43168228], step_size = 0.01\n",
            "Iteration 37: x = [-2.131769    1.44872528], gradient = [ 0.14873597 -3.36608745], step_size = 0.01\n",
            "Iteration 38: x = [-2.13325636  1.48238615], gradient = [ 0.14560684 -3.30174042], step_size = 0.01\n",
            "Iteration 39: x = [-2.13471243  1.51540356], gradient = [ 0.14255036 -3.23861775], step_size = 0.01\n",
            "Iteration 40: x = [-2.13613793  1.54778973], gradient = [ 0.13956419 -3.1766964 ], step_size = 0.01\n",
            "Iteration 41: x = [-2.13753357  1.5795567 ], gradient = [ 0.13664627 -3.11595376], step_size = 0.01\n",
            "Iteration 42: x = [-2.13890004  1.61071623], gradient = [ 0.1337947  -3.05636761], step_size = 0.01\n",
            "Iteration 43: x = [-2.14023798  1.64127991], gradient = [ 0.13100768 -2.99791615], step_size = 0.01\n",
            "Iteration 44: x = [-2.14154806  1.67125907], gradient = [ 0.1282835  -2.94057798], step_size = 0.01\n",
            "Iteration 45: x = [-2.1428309   1.70066485], gradient = [ 0.12562052 -2.88433209], step_size = 0.01\n",
            "Iteration 46: x = [-2.1440871   1.72950817], gradient = [ 0.12301716 -2.82915786], step_size = 0.01\n",
            "Iteration 47: x = [-2.14531727  1.75779975], gradient = [ 0.12047189 -2.77503504], step_size = 0.01\n",
            "Iteration 48: x = [-2.14652199  1.7855501 ], gradient = [ 0.11798324 -2.72194378], step_size = 0.01\n",
            "Iteration 49: x = [-2.14770182  1.81276954], gradient = [ 0.11554977 -2.66986457], step_size = 0.01\n",
            "Iteration 50: x = [-2.14885732  1.83946819], gradient = [ 0.1131701  -2.61877827], step_size = 0.01\n",
            "Iteration 51: x = [-2.14998902  1.86565597], gradient = [ 0.11084288 -2.56866611], step_size = 0.01\n",
            "Iteration 52: x = [-2.15109745  1.89134263], gradient = [ 0.10856681 -2.51950965], step_size = 0.01\n",
            "Iteration 53: x = [-2.15218312  1.91653773], gradient = [ 0.10634063 -2.47129079], step_size = 0.01\n",
            "Iteration 54: x = [-2.15324653  1.94125063], gradient = [ 0.1041631  -2.42399179], step_size = 0.01\n",
            "Iteration 55: x = [-2.15428816  1.96549055], gradient = [ 0.10203303 -2.37759521], step_size = 0.01\n",
            "Iteration 56: x = [-2.15530849  1.9892665 ], gradient = [ 0.09994927 -2.33208397], step_size = 0.01\n",
            "Iteration 57: x = [-2.15630798  2.01258734], gradient = [ 0.0979107  -2.28744127], step_size = 0.01\n",
            "Iteration 58: x = [-2.15728709  2.03546176], gradient = [ 0.09591622 -2.24365066], step_size = 0.01\n",
            "Iteration 59: x = [-2.15824625  2.05789826], gradient = [ 0.09396477 -2.20069597], step_size = 0.01\n",
            "Iteration 60: x = [-2.1591859   2.07990522], gradient = [ 0.09205533 -2.15856135], step_size = 0.01\n",
            "Iteration 61: x = [-2.16010645  2.10149084], gradient = [ 0.09018689 -2.11723123], step_size = 0.01\n",
            "Iteration 62: x = [-2.16100832  2.12266315], gradient = [ 0.08835848 -2.07669034], step_size = 0.01\n",
            "Iteration 63: x = [-2.1618919   2.14343005], gradient = [ 0.08656915 -2.03692371], step_size = 0.01\n",
            "Iteration 64: x = [-2.1627576   2.16379929], gradient = [ 0.08481799 -1.99791661], step_size = 0.01\n",
            "Iteration 65: x = [-2.16360578  2.18377845], gradient = [ 0.0831041  -1.95965464], step_size = 0.01\n",
            "Iteration 66: x = [-2.16443682  2.203375  ], gradient = [ 0.08142661 -1.92212363], step_size = 0.01\n",
            "Iteration 67: x = [-2.16525108  2.22259624], gradient = [ 0.07978467 -1.88530969], step_size = 0.01\n",
            "Iteration 68: x = [-2.16604893  2.24144933], gradient = [ 0.07817746 -1.84919919], step_size = 0.01\n",
            "Iteration 69: x = [-2.1668307   2.25994133], gradient = [ 0.07660418 -1.81377876], step_size = 0.01\n",
            "Iteration 70: x = [-2.16759675  2.27807911], gradient = [ 0.07506405 -1.77903526], step_size = 0.01\n",
            "Iteration 71: x = [-2.16834739  2.29586947], gradient = [ 0.07355631 -1.74495584], step_size = 0.01\n",
            "Iteration 72: x = [-2.16908295  2.31331902], gradient = [ 0.07208021 -1.71152785], step_size = 0.01\n",
            "Iteration 73: x = [-2.16980375  2.3304343 ], gradient = [ 0.07063505 -1.6787389 ], step_size = 0.01\n",
            "Iteration 74: x = [-2.1705101   2.34722169], gradient = [ 0.06922011 -1.64657682], step_size = 0.01\n",
            "Iteration 75: x = [-2.1712023   2.36368746], gradient = [ 0.06783471 -1.61502969], step_size = 0.01\n",
            "Iteration 76: x = [-2.17188065  2.37983776], gradient = [ 0.06647819 -1.58408579], step_size = 0.01\n",
            "Iteration 77: x = [-2.17254543  2.39567862], gradient = [ 0.0651499  -1.55373363], step_size = 0.01\n",
            "Iteration 78: x = [-2.17319693  2.41121595], gradient = [ 0.0638492  -1.52396196], step_size = 0.01\n",
            "Iteration 79: x = [-2.17383542  2.42645557], gradient = [ 0.06257549 -1.4947597 ], step_size = 0.01\n",
            "Iteration 80: x = [-2.17446118  2.44140317], gradient = [ 0.06132815 -1.46611602], step_size = 0.01\n",
            "Iteration 81: x = [-2.17507446  2.45606433], gradient = [ 0.06010661 -1.43802026], step_size = 0.01\n",
            "Iteration 82: x = [-2.17567553  2.47044453], gradient = [ 0.05891028 -1.41046199], step_size = 0.01\n",
            "Iteration 83: x = [-2.17626463  2.48454915], gradient = [ 0.05773863 -1.38343096], step_size = 0.01\n",
            "Iteration 84: x = [-2.17684201  2.49838346], gradient = [ 0.05659109 -1.35691711], step_size = 0.01\n",
            "Iteration 85: x = [-2.17740793  2.51195263], gradient = [ 0.05546715 -1.33091059], step_size = 0.01\n",
            "Iteration 86: x = [-2.1779626   2.52526174], gradient = [ 0.05436628 -1.30540172], step_size = 0.01\n",
            "Iteration 87: x = [-2.17850626  2.53831575], gradient = [ 0.05328798 -1.28038101], step_size = 0.01\n",
            "Iteration 88: x = [-2.17903914  2.55111956], gradient = [ 0.05223177 -1.25583915], step_size = 0.01\n",
            "Iteration 89: x = [-2.17956146  2.56367796], gradient = [ 0.05119715 -1.231767  ], step_size = 0.01\n",
            "Iteration 90: x = [-2.18007343  2.57599563], gradient = [ 0.05018366 -1.20815561], step_size = 0.01\n",
            "Iteration 91: x = [-2.18057527  2.58807718], gradient = [ 0.04919085 -1.18499617], step_size = 0.01\n",
            "Iteration 92: x = [-2.18106717  2.59992714], gradient = [ 0.04821827 -1.16228006], step_size = 0.01\n",
            "Iteration 93: x = [-2.18154936  2.61154994], gradient = [ 0.04726547 -1.13999882], step_size = 0.01\n",
            "Iteration 94: x = [-2.18202201  2.62294993], gradient = [ 0.04633205 -1.11814416], step_size = 0.01\n",
            "Iteration 95: x = [-2.18248533  2.63413137], gradient = [ 0.04541757 -1.09670792], step_size = 0.01\n",
            "Iteration 96: x = [-2.18293951  2.64509845], gradient = [ 0.04452164 -1.07568211], step_size = 0.01\n",
            "Iteration 97: x = [-2.18338472  2.65585527], gradient = [ 0.04364387 -1.0550589 ], step_size = 0.01\n",
            "Iteration 98: x = [-2.18382116  2.66640586], gradient = [ 0.04278385 -1.0348306 ], step_size = 0.01\n",
            "Iteration 99: x = [-2.184249    2.67675417], gradient = [ 0.04194122 -1.01498966], step_size = 0.01\n",
            "Final x: [-2.18466841  2.68690407]\n",
            "Function values: [19.0, 18.418967009999996, 17.59369613131624, 16.4302090061154, 14.81344450988377, 12.619297764701319, 9.746811886218385, 6.178515834552269, 2.0609697205841964, -2.241079713859561, -6.201599757110506, -9.343331609878067, -11.478479917540215, -12.757008451478796, -13.487392157631858, -13.93705260533443, -14.262858947883839, -14.536665107080307, -14.786578432871716, -15.02266325872628, -15.248451846677852, -15.465283657036629, -15.673789804158162, -15.874373350518486, -16.067359849454437, -16.25304379670383, -16.431702981696063, -16.603603075293982, -16.768999268038044, -16.928137015564893, -17.0812525079414, -17.22857304736985, -17.370317389610683, -17.506696066000917, -17.637911691468567, -17.764159260494374, -17.885626431915036, -18.002493803123887, -18.114935174106677, -18.223117801701207, -18.327202644443492, -18.427344598345563, -18.523692723935923, -18.616390464881206, -18.7055758584958, -18.791381738435426, -18.87393592985998, -18.953361437340952, -19.029776625778798, -19.103295394586393, -19.174027345385504, -19.242077943454483, -19.307548673156653, -19.370537187571, -19.431137452538533, -19.48943988533017, -19.54553148813457, -19.59949597655704, -19.651413903314094, -19.701362777301046, -19.74941717820413, -19.795648866821967, -19.840126891255515, -19.882917689119743, -19.924085185924543, -19.963690889767328, -20.001793982474226, -20.038451407322057, -20.073717953468094, -20.107646337210475, -20.140287280196983, -20.171689584696196, -20.201900206040328, -20.23096432234539, -20.258925401610277, -20.285825266292562, -20.311704155455395, -20.336600784576152, -20.36055240310434, -20.383594849852948, -20.405762606304215, -20.427088847908006, -20.447605493447988, -20.467343252547774, -20.486331671386992, -20.50459917669435, -20.522173118082023, -20.539079808784077, -20.555344564858355, -20.570991742909758, -20.586044776390413, -20.600526210530106, -20.61445773594847, -20.627860220998507, -20.640753742889093, -20.653157617632353, -20.665090428860204, -20.676570055552506, -20.687613698717772, -20.69823790706595, -20.708458601711126]\n",
            "X values: [array([0., 0.]), array([-0.07,  0.02]), array([-0.15423928,  0.041     ]), array([-0.25504667,  0.06326479]), array([-0.37470622,  0.08710042]), array([-0.51507291,  0.11285254]), array([-0.67691958,  0.14089695]), array([-0.8589677,  0.1716174]), array([-1.05670809,  0.20536441]), array([-1.26145988,  0.24239128]), array([-1.46056776,  0.28277265]), array([-1.6397083 ,  0.32632855]), array([-1.7871736 ,  0.37259615]), array([-1.89791201,  0.4208877 ]), array([-1.97439359,  0.47042818]), array([-2.02386835,  0.52050749]), array([-2.0545768 ,  0.57057471]), array([-2.07334684,  0.62025475]), array([-2.08494488,  0.66931659]), array([-2.09238052,  0.71762916]), array([-2.09744514,  0.76512419]), array([-2.10116875,  0.8117706 ]), array([-2.10413176,  0.85755857]), array([-2.10665734,  0.90249003]), array([-2.10892421,  0.94657338]), array([-2.11103089,  0.98982039]), array([-2.11303167,  1.0322446 ]), array([-2.11495657,  1.07386035]), array([-2.11682233,  1.11468227]), array([-2.11863846,  1.15472507]), array([-2.12041051,  1.19400334]), array([-2.12214189,  1.23253148]), array([-2.12383484,  1.27032369]), array([-2.12549095,  1.30739391]), array([-2.12711145,  1.34375585]), array([-2.12869736,  1.37942297]), array([-2.1302496 ,  1.41440845]), array([-2.131769  ,  1.44872528]), array([-2.13325636,  1.48238615]), array([-2.13471243,  1.51540356]), array([-2.13613793,  1.54778973]), array([-2.13753357,  1.5795567 ]), array([-2.13890004,  1.61071623]), array([-2.14023798,  1.64127991]), array([-2.14154806,  1.67125907]), array([-2.1428309 ,  1.70066485]), array([-2.1440871 ,  1.72950817]), array([-2.14531727,  1.75779975]), array([-2.14652199,  1.7855501 ]), array([-2.14770182,  1.81276954]), array([-2.14885732,  1.83946819]), array([-2.14998902,  1.86565597]), array([-2.15109745,  1.89134263]), array([-2.15218312,  1.91653773]), array([-2.15324653,  1.94125063]), array([-2.15428816,  1.96549055]), array([-2.15530849,  1.9892665 ]), array([-2.15630798,  2.01258734]), array([-2.15728709,  2.03546176]), array([-2.15824625,  2.05789826]), array([-2.1591859 ,  2.07990522]), array([-2.16010645,  2.10149084]), array([-2.16100832,  2.12266315]), array([-2.1618919 ,  2.14343005]), array([-2.1627576 ,  2.16379929]), array([-2.16360578,  2.18377845]), array([-2.16443682,  2.203375  ]), array([-2.16525108,  2.22259624]), array([-2.16604893,  2.24144933]), array([-2.1668307 ,  2.25994133]), array([-2.16759675,  2.27807911]), array([-2.16834739,  2.29586947]), array([-2.16908295,  2.31331902]), array([-2.16980375,  2.3304343 ]), array([-2.1705101 ,  2.34722169]), array([-2.1712023 ,  2.36368746]), array([-2.17188065,  2.37983776]), array([-2.17254543,  2.39567862]), array([-2.17319693,  2.41121595]), array([-2.17383542,  2.42645557]), array([-2.17446118,  2.44140317]), array([-2.17507446,  2.45606433]), array([-2.17567553,  2.47044453]), array([-2.17626463,  2.48454915]), array([-2.17684201,  2.49838346]), array([-2.17740793,  2.51195263]), array([-2.1779626 ,  2.52526174]), array([-2.17850626,  2.53831575]), array([-2.17903914,  2.55111956]), array([-2.17956146,  2.56367796]), array([-2.18007343,  2.57599563]), array([-2.18057527,  2.58807718]), array([-2.18106717,  2.59992714]), array([-2.18154936,  2.61154994]), array([-2.18202201,  2.62294993]), array([-2.18248533,  2.63413137]), array([-2.18293951,  2.64509845]), array([-2.18338472,  2.65585527]), array([-2.18382116,  2.66640586]), array([-2.184249  ,  2.67675417]), array([-2.18466841,  2.68690407])]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def gd(f, df, x0, step_size_fn, max_iter):\n",
        "    \"\"\"\n",
        "    Generic gradient descent function.\n",
        "\n",
        "    Parameters:\n",
        "    - f: Function to minimize. It takes a column vector and returns a scalar.\n",
        "    - df: Gradient of function f. It takes a column vector and returns the gradient.\n",
        "    - x0: Initial value of x, a column vector.\n",
        "    - step_size_fn: Function that takes the iteration index and returns the step size.\n",
        "    - max_iter: Number of iterations to perform.\n",
        "\n",
        "    Returns:\n",
        "    - Tuple (x, fs, xs) where:\n",
        "        - x: Value at the final step.\n",
        "        - fs: List of function values during all iterations.\n",
        "        - xs: List of x values during all iterations.\n",
        "    \"\"\"\n",
        "    # Initialize lists to store the values of x and f(x)\n",
        "    xs = [x0]\n",
        "    fs = [f(x0)]\n",
        "\n",
        "    # Start with the initial value of x\n",
        "    x = x0\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        # Get the current step size\n",
        "        step_size = step_size_fn(i)\n",
        "\n",
        "        # Compute the gradient\n",
        "        gradient = df(x)\n",
        "\n",
        "        # Debugging information\n",
        "        print(f\"Iteration {i}: x = {x}, gradient = {gradient}, step_size = {step_size}\")\n",
        "\n",
        "        # Update x using gradient descent\n",
        "        try:\n",
        "            # Clamp x to a reasonable range to avoid overflow\n",
        "            x = x - step_size * gradient\n",
        "            x = np.clip(x, -1e10, 1e10)  # Clamping to prevent extreme values\n",
        "        except OverflowError:\n",
        "            print(f\"OverflowError encountered at iteration {i}\")\n",
        "            break\n",
        "\n",
        "        # Store the new values of x and f(x)\n",
        "        try:\n",
        "            fs.append(f(x))\n",
        "            xs.append(x)\n",
        "        except ValueError as e:\n",
        "            print(f\"ValueError encountered while evaluating function at iteration {i}: {e}\")\n",
        "            break\n",
        "\n",
        "    return (x, fs, xs)\n",
        "\n",
        "# Define a smaller step size function\n",
        "def constant_step_size(iter_index):\n",
        "    return 0.01  # Smaller step size\n",
        "\n",
        "# Define the functions f2 and df2\n",
        "def f2(v):\n",
        "    x = float(v[0])\n",
        "    y = float(v[1])\n",
        "\n",
        "    # Avoid extremely large values\n",
        "    if np.abs(x) > 1e5 or np.abs(y) > 1e5:\n",
        "        x = np.clip(x, -1e5, 1e5)\n",
        "        y = np.clip(y, -1e5, 1e5)\n",
        "\n",
        "    term1 = (x - 2.) * (x - 3.) * (x + 3.) * (x + 1.)\n",
        "    term2 = (x + y - 1)**2\n",
        "\n",
        "    return term1 + term2\n",
        "\n",
        "def df2(v):\n",
        "    x = float(v[0])\n",
        "    y = float(v[1])\n",
        "\n",
        "    # Compute the partial derivatives\n",
        "    df_dx = (-3. + x) * (-2. + x) * (1. + x) + \\\n",
        "            (-3. + x) * (-2. + x) * (3. + x) + \\\n",
        "            (-3. + x) * (1. + x) * (3. + x) + \\\n",
        "            (-2. + x) * (1. + x) * (3. + x) + \\\n",
        "            2 * (-1. + x + y)\n",
        "\n",
        "    df_dy = 2 * (-1. + x + y)\n",
        "\n",
        "    return np.array([df_dx, df_dy])\n",
        "\n",
        "# Example usage\n",
        "x0_f2 = np.array([0.0, 0.0])  # Initial point for f2\n",
        "\n",
        "try:\n",
        "    final_x, fs, xs = gd(f2, df2, x0_f2, constant_step_size, 100)\n",
        "    print(\"Final x:\", final_x)\n",
        "    print(\"Function values:\", fs)\n",
        "    print(\"X values:\", xs)\n",
        "except Exception as e:\n",
        "    print(\"Error during gradient descent:\", e)\n"
      ]
    }
  ]
}