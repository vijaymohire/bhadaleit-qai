{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"15rvEbDrgQ7BNCeTgsKRj8YSef-zEetdt","timestamp":1731849828484},{"file_id":"1TlNbIK-3cGwANXK1vWkIUo_LrCLmD-4N","timestamp":1731849455075}],"authorship_tag":"ABX9TyN6UQHgbKlHWfRydDo0/Vyl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **Summary of Kautz's Taxonomy of Neuro-symbolic Architectures:**\n","\n","1. Neural Network-Only Systems\n","2. Symbolic-Only Systems\n","3. End-to-End Neural Networks with Structured Output\n","4. Neuro-Symbolic Hybrid Architectures\n","5. Deep Neuro-Symbolic Systems\n","\n","Kautz's taxonomy provides a useful framework to understand the different ways that neural and symbolic systems can be integrated to achieve more flexible, general AI. These approaches range from simpler forms of combination, like neural networks augmented by symbolic constraints, to more sophisticated forms, like meta-level integration where the system reflects on and improves its own reasoning and learning processes. As neuro-symbolic AI continues to evolve, these categories will likely become more refined, and new approaches may emerge, bridging the gap toward ***Artificial General Intelligence (AGI).***\n","\n","\n","\n","\n","\n"],"metadata":{"id":"TowtxFf-PjZT"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jbQYCpuROhxU","executionInfo":{"status":"ok","timestamp":1731850898594,"user_tz":-330,"elapsed":17675,"user":{"displayName":"Vijayananda Mohire","userId":"16579720561741340503"}},"outputId":"c5453ef3-b35c-4e63-b0d2-b5c92d2e5e45"},"outputs":[{"output_type":"stream","name":"stderr","text":["You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Generated Logical Form: Translate the sentence into logical form: Who is the president of the USA?\n"]}],"source":["# 3.End-to-End Neural Networks with Structured Output\n","\n","# This is an example where we use a neural network for semantic parsing, turning sentences into structured logical forms (like a decision tree or\n","#simple SQL-like query). We use a basic transformer model from Hugging Face's transformers library.\n","\n","from transformers import T5ForConditionalGeneration, T5Tokenizer\n","\n","# Load pre-trained T5 model for text-to-text tasks\n","model = T5ForConditionalGeneration.from_pretrained('t5-small')\n","tokenizer = T5Tokenizer.from_pretrained('t5-small')\n","\n","# Sample input sentence\n","input_text = \"Translate the sentence into logical form: Who is the president of the USA?\"\n","\n","# Tokenize and generate output\n","input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n","output_ids = model.generate(input_ids)\n","\n","# Decode the generated output\n","output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","print(\"Generated Logical Form:\", output_text)\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","source":["**Code sample generated by ChatGPT, executed by Bhadale IT**"],"metadata":{"id":"avxqeBUCdL0V"}}]}