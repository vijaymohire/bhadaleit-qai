{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1N_s7roBG-CC_0SB1nHYSECtuTOcEAQvf","timestamp":1731851729137},{"file_id":"1WvHWNb9dYOHG3yL1cRqk-8BRNdXfDO4q","timestamp":1731850993034},{"file_id":"15rvEbDrgQ7BNCeTgsKRj8YSef-zEetdt","timestamp":1731849828484},{"file_id":"1TlNbIK-3cGwANXK1vWkIUo_LrCLmD-4N","timestamp":1731849455075}],"authorship_tag":"ABX9TyNgmZIQ/qZiyNpVTK8+jRk5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **Summary of Kautz's Taxonomy of Neuro-symbolic Architectures:**\n","\n","1. Neural Network-Only Systems\n","2. Symbolic-Only Systems\n","3. End-to-End Neural Networks with Structured Output\n","4. Neuro-Symbolic Hybrid Architectures\n","5. Deep Neuro-Symbolic Systems\n","\n","Kautz's taxonomy provides a useful framework to understand the different ways that neural and symbolic systems can be integrated to achieve more flexible, general AI. These approaches range from simpler forms of combination, like neural networks augmented by symbolic constraints, to more sophisticated forms, like meta-level integration where the system reflects on and improves its own reasoning and learning processes. As neuro-symbolic AI continues to evolve, these categories will likely become more refined, and new approaches may emerge, bridging the gap toward ***Artificial General Intelligence (AGI).***\n","\n","\n","\n","\n","\n"],"metadata":{"id":"TowtxFf-PjZT"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jbQYCpuROhxU","executionInfo":{"status":"ok","timestamp":1731852098001,"user_tz":-330,"elapsed":137310,"user":{"displayName":"Vijayananda Mohire","userId":"16579720561741340503"}},"outputId":"b706b6ff-a82d-4824-cf34-4d58e872237a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 35.3MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28.9k/28.9k [00:00<00:00, 1.13MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1.65M/1.65M [00:00<00:00, 8.09MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4.54k/4.54k [00:00<00:00, 2.42MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Epoch 1, Loss: 0.34446975588798523, Learning Rate: 0.01\n","Epoch 2, Loss: 0.10160618275403976, Learning Rate: 0.01\n","Epoch 3, Loss: 0.39203953742980957, Learning Rate: 0.01\n","Epoch 4, Loss: 0.11286155879497528, Learning Rate: 0.01\n","Epoch 5, Loss: 0.2190706431865692, Learning Rate: 0.01\n"]}],"source":["# 5.Deep Neuro-Symbolic Systems\n","\n","# In this example, we simulate a deep neuro-symbolic system where we have both neural network processing (e.g., feature extraction) and\n","# a meta-cognitive layer that adjusts the system's behavior based on the results. We'll use a simple neural network to classify images and add\n","# a meta-cognitive layer to adjust the learning rate dynamically based on performance.\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","# Define a simple neural network\n","class SimpleNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleNN, self).__init__()\n","        self.fc1 = nn.Linear(28 * 28, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 10)  # 10 classes for MNIST\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        return self.fc3(x)\n","\n","# Meta-cognitive layer: adjusts learning rate based on loss\n","def adjust_learning_rate(optimizer, loss, threshold=0.5):\n","    if loss.item() > threshold:\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = 0.001  # Lower learning rate\n","    else:\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = 0.01  # Higher learning rate\n","\n","# Load dataset\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n","\n","# Initialize the model and optimizer\n","model = SimpleNN()\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Training loop with meta-cognitive adjustment\n","for epoch in range(5):\n","    for data, target in trainloader:\n","        data = data.view(-1, 28 * 28)  # Flatten image\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Meta-cognitive layer: Adjust learning rate based on loss\n","        adjust_learning_rate(optimizer, loss)\n","\n","    print(f\"Epoch {epoch+1}, Loss: {loss.item()}, Learning Rate: {optimizer.param_groups[0]['lr']}\")\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","source":["**Code sample generated by ChatGPT, executed by Bhadale IT**"],"metadata":{"id":"orCywfiZcnDO"}}]}